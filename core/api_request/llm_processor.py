# llm_processor.py
import asyncio
import json
import logging
import time
from typing import List, Dict, Optional
import openai
from openai import AsyncOpenAI
import os
from dataclasses import dataclass

logger = logging.getLogger(__name__)


@dataclass
class LLMResponse:
    """LLM ì‘ë‹µ ë°ì´í„° í´ë˜ìŠ¤"""
    original_text: str
    processed_text: str
    timestamp: float
    session_id: str
    processing_time: float
    turn_number: int  # ëŒ€í™” í„´ ë²ˆí˜¸ ì¶”ê°€


class LLMProcessor:
    """LLM ì„œë²„ ì²˜ë¦¬ë§Œ ë‹´ë‹¹í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self, session_id: str, api_key: Optional[str] = None, base_url: Optional[str] = None):
        self.session_id = session_id
        self.base_url = os.getenv('LLM_BASE_URL')
        if self.base_url is None:
            raise ValueError("LLM_BASE_URL í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ")
        self.api_key = api_key or os.getenv('LLM_API_KEY', 'dummy-key')  # ì¼ë¶€ ë¡œì»¬ ì„œë²„ëŠ” API í‚¤ê°€ í•„ìš”ì—†ìŒ

        self.client = AsyncOpenAI(
            api_key=self.api_key,
            base_url=self.base_url
        )
        self.processed_responses: List[LLMResponse] = []

        # LLM ì„¤ì •
        self.model = "LUXIA"
#         self.system_prompt = """
# You are a professional education advisor and mentor. Your mission is guide the student to improve in their studies.
# You must check their interests (subjects, hobbies, exams). Give relevant information on the subject.
# Give appropriate advice when needed. Be polite and helpful.
# ì£¼ì˜: ë‹¹ì‹ ì€ í•œêµ­ì¸ì´ê³ , í•™ìƒë„ í•œêµ­ì¸ì´ê¸° ë•Œë¬¸ì—, ë‹µë³€ì€ ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ í•´ì•¼ í•©ë‹ˆë‹¤. ìœ„ ì¡°ê±´ì„ ë§ì¶”ë©´ì„œë„, ë‹µë³€ì€ í•œêµ­ì–´ë¡œ í•´ì•¼ í•©ë‹ˆë‹¤.
# ê°€ëŠ¥í•œ ì „ë¬¸ìš©ì–´ë“¤ì€ êµ­ë¬¸ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
#         """.strip()
        self.system_prompt = """ë‹¹ì‹ ì€ ì£¼ì–´ì§„ [ì§ˆë¬¸]ì— ëŒ€í•œ ë‹µë³€ì„ ì‘ë‹µí•˜ëŠ” ì†”íŠ¸ë£©ìŠ¤ ë©”íƒ€íœ´ë¨¼ êµìˆ˜ë‹˜ ì…ë‹ˆë‹¤. [ê·œì¹™]ì„ ì¤€ìˆ˜í•˜ì—¬ ë‹µí•´ì£¼ì„¸ìš”.

<ê·œì¹™>
1. "ë‹¹ì—°í•˜ì£ ", "ë¬¸ì„œì— ë”°ë¥´ë©´"ê³¼ ê°™ì€ ë‹¨ì–´ë¡œ ì‹œì‘í•˜ì—¬ ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”.
2. ì‘ë‹µì„ ìƒì„±í•  ë•Œ, ì¸ì‚¬ë§ì„ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”. ì•„ë˜ [ì§ˆë¬¸]ì— ëŒ€í•œ ë‹µë§Œ í•©ë‹ˆë‹¤.
3. ì‘ë‹µì„ ìƒì„±í•  ë•ŒëŠ” êµ¬ì–´ì²´ë¡œ ë‹µë³€í•´ì£¼ì„¸ìš”. ì´ëŠ” ë‹µë³€ì´ ë°œí™”ë¡œ ì‚¬ìš©ë˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.
4. ë‹µë³€ì€ ê³µì†í•œ í•œêµ­ì–´ë¡œ í•©ë‹ˆë‹¤.
5. ê°€ëŠ¥í•œ ê²½ìš° ë‹µë³€ì´ 4ë¬¸ì¥ ì •ë„ë¡œ ë˜ë„ë¡ í•´ì£¼ì„¸ìš”
"""

        # ë©€í‹°í„´ ëŒ€í™”ë¥¼ ìœ„í•œ ëŒ€í™” íˆìŠ¤í† ë¦¬
        self.conversation_history: List[Dict[str, str]] = []
        self.max_history_turns = int(os.getenv('MAX_HISTORY_TURNS', '10'))  # ìµœëŒ€ íˆìŠ¤í† ë¦¬ í„´ ìˆ˜
        self.turn_counter = 0

        # ìŠ¤íŠ¸ë¦¬ë° ì½œë°± ì‹œìŠ¤í…œ
        self.streaming_callbacks: List[callable] = []

        logger.info(f"[LLM] ì„¸ì…˜ {self.session_id[:4]}: ë©€í‹°í„´ ëŒ€í™” í”„ë¡œì„¸ì„œ ì´ˆê¸°í™” (ìµœëŒ€ íˆìŠ¤í† ë¦¬: {self.max_history_turns}í„´)")

    def add_streaming_callback(self, callback: callable):
        """ìŠ¤íŠ¸ë¦¬ë° ì½œë°± í•¨ìˆ˜ ì¶”ê°€"""
        self.streaming_callbacks.append(callback)
        logger.debug(f"[LLM] ì„¸ì…˜ {self.session_id[:4]}: ìŠ¤íŠ¸ë¦¬ë° ì½œë°± í•¨ìˆ˜ ì¶”ê°€ë¨")

    def remove_streaming_callback(self, callback: callable):
        """ìŠ¤íŠ¸ë¦¬ë° ì½œë°± í•¨ìˆ˜ ì œê±°"""
        if callback in self.streaming_callbacks:
            self.streaming_callbacks.remove(callback)
            logger.debug(f"[LLM] ì„¸ì…˜ {self.session_id[:4]}: ìŠ¤íŠ¸ë¦¬ë° ì½œë°± í•¨ìˆ˜ ì œê±°ë¨")

    def _prepare_messages(self, text: str) -> List[Dict[str, str]]:
        """ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ ì¤€ë¹„ (ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ + ëŒ€í™” íˆìŠ¤í† ë¦¬ + í˜„ì¬ ì…ë ¥)"""
        messages = []

        # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¶”ê°€
        if self.system_prompt:
            messages.append({"role": "system", "content": self.system_prompt})

        # ëŒ€í™” íˆìŠ¤í† ë¦¬ ì¶”ê°€
        messages.extend(self.conversation_history)

        # í˜„ì¬ ì‚¬ìš©ì ì…ë ¥ ì¶”ê°€
        messages.append({"role": "user", "content": f"[ì§ˆë¬¸]\n{text}"})

        return messages

    def _add_to_history(self, user_text: str, assistant_text: str):
        """ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€"""
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        self.conversation_history.append({"role": "user", "content": user_text})
        # ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ ì¶”ê°€
        self.conversation_history.append({"role": "assistant", "content": assistant_text})

        # íˆìŠ¤í† ë¦¬ ê¸¸ì´ ê´€ë¦¬ (ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì œì™¸í•˜ê³  user-assistant ìŒìœ¼ë¡œ ê³„ì‚°)
        while len(self.conversation_history) > self.max_history_turns * 2:
            # ê°€ì¥ ì˜¤ë˜ëœ user-assistant ìŒ ì œê±°
            self.conversation_history.pop(0)  # user ë©”ì‹œì§€ ì œê±°
            if self.conversation_history:  # assistant ë©”ì‹œì§€ë„ ìˆë‹¤ë©´ ì œê±°
                self.conversation_history.pop(0)
            logger.debug(f"[LLM] ì„¸ì…˜ {self.session_id[:4]}: ì˜¤ë˜ëœ ëŒ€í™” íˆìŠ¤í† ë¦¬ ì œê±° (í˜„ì¬ ê¸¸ì´: {len(self.conversation_history)})")

    async def process_text(self, text: str) -> Optional[LLMResponse]:
        """í…ìŠ¤íŠ¸ë¥¼ OpenAI í˜¸í™˜ LLMìœ¼ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ (ë©€í‹°í„´ ì§€ì›)"""
        if not text.strip():
            return None

        try:
            self.turn_counter += 1
            start_time = time.time()

            # ë©”ì‹œì§€ ì¤€ë¹„
            messages = self._prepare_messages(text)

            logger.info(
                f"ğŸ¤– [LLM] ì„¸ì…˜ {self.session_id[:4]}: LLM ì„œë²„ ìŠ¤íŠ¸ë¦¬ë° ìš”ì²­ ì‹œì‘ (í„´ #{self.turn_counter}, íˆìŠ¤í† ë¦¬: {len(self.conversation_history)}ê°œ)")

            # ìŠ¤íŠ¸ë¦¬ë° LLM API í˜¸ì¶œ
            stream = await self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                max_tokens=200,
                temperature=0.3,
                stream=True,  # ìŠ¤íŠ¸ë¦¬ë° í™œì„±í™”
                timeout=6000.0
            )

            # ìŠ¤íŠ¸ë¦¬ë° ê²°ê³¼ ìˆ˜ì§‘
            streaming_text = ""
            chunk_count = 0
            async for chunk in stream:
                chunk_count += 1
                if chunk.choices and chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    streaming_text += content
                    # ìŠ¤íŠ¸ë¦¬ë° ì½œë°± í˜¸ì¶œ (ì‹¤ì‹œê°„ ì¡°ê±´ ì²´í¬ìš©)
                    if self.streaming_callbacks:  # ì½œë°±ì´ ìˆì„ ë•Œë§Œ
                        for i, callback in enumerate(self.streaming_callbacks):
                            try:
                                if asyncio.iscoroutinefunction(callback):
                                    await callback(content, streaming_text, chunk_count)
                                else:
                                    callback(content, streaming_text, chunk_count)
                            except Exception as e:
                                logger.error(f"[LLM] ìŠ¤íŠ¸ë¦¬ë° ì½œë°± {i + 1} ì˜¤ë¥˜: {e}")
                    else:
                        logger.warning(f"[LLM] ì²­í¬ {chunk_count}: ë“±ë¡ëœ ì½œë°±ì´ ì—†ìŒ!")

                else:
                    # ì²˜ìŒ 5ê°œ ë¹ˆ ì²­í¬ë§Œ ë¡œê·¸
                    if chunk_count <= 5:
                        logger.info(f"[LLM-EMPTY] ì²­í¬ {chunk_count}: ë‚´ìš© ì—†ìŒ")

            processing_time = time.time() - start_time
            final_response = streaming_text.strip()

            if not final_response:
                logger.warning(f"[LLM] ì„¸ì…˜ {self.session_id[:4]}: ë¹ˆ ì‘ë‹µ ìˆ˜ì‹ ")
                return None

            # ëŒ€í™” íˆìŠ¤í† ë¦¬ì— ì¶”ê°€
            self._add_to_history(text, final_response)

            # ì‘ë‹µ ê°ì²´ ìƒì„±
            llm_response = LLMResponse(
                original_text=text,
                processed_text=final_response,
                timestamp=time.time(),
                session_id=self.session_id,
                processing_time=processing_time,
                turn_number=self.turn_counter
            )

            # ê²°ê³¼ ì €ì¥
            self.processed_responses.append(llm_response)

            formatted_time = time.strftime("%H:%M:%S", time.localtime(llm_response.timestamp))
            logger.info(f"âœ… [LLM-{self.session_id[:4]}] [{formatted_time}] í„´ #{self.turn_counter} ìŠ¤íŠ¸ë¦¬ë° ì™„ë£Œ "
                        f"(íˆìŠ¤í† ë¦¬: {len(self.conversation_history)}ê°œ)")

            return llm_response

        except asyncio.TimeoutError:
            logger.error(f"[LLM] ì„¸ì…˜ {self.session_id[:4]}: LLM ì„œë²„ ìŠ¤íŠ¸ë¦¬ë° íƒ€ì„ì•„ì›ƒ (í„´ #{self.turn_counter})")
            return None

        except openai.RateLimitError:
            logger.error(f"[LLM] ì„¸ì…˜ {self.session_id[:4]}: LLM ì„œë²„ ìš”ì²­ í•œë„ ì´ˆê³¼ (í„´ #{self.turn_counter})")
            return None

        except openai.APIError as e:
            logger.error(f"[LLM] ì„¸ì…˜ {self.session_id[:4]}: LLM ì„œë²„ API ì˜¤ë¥˜ (í„´ #{self.turn_counter}) - {e}")
            return None

        except Exception as e:
            logger.error(f"[LLM] ì„¸ì…˜ {self.session_id[:4]}: LLM ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ì˜¤ë¥˜ (í„´ #{self.turn_counter}) - {e}")
            return None

    async def cleanup(self):
        """LLM í”„ë¡œì„¸ì„œ ì •ë¦¬"""
        # ì½œë°± ì •ë¦¬
        self.streaming_callbacks.clear()

        # íˆìŠ¤í† ë¦¬ ì •ë¦¬
        self.conversation_history.clear()

        logger.info(f"[LLM] ì„¸ì…˜ {self.session_id[:4]}: í”„ë¡œì„¸ì„œ ì •ë¦¬ ì™„ë£Œ "
                    f"(ì´ ì²˜ë¦¬: {len(self.processed_responses)}ê°œ)")


class LLMManager:
    """ì—¬ëŸ¬ ì„¸ì…˜ì˜ LLM í”„ë¡œì„¸ì„œ ê´€ë¦¬"""

    def __init__(self):
        self.processors: Dict[str, LLMProcessor] = {}

    def create_processor(self, session_id: str) -> LLMProcessor:
        """ìƒˆ LLM í”„ë¡œì„¸ì„œ ìƒì„±"""
        if session_id in self.processors:
            logger.warning(f"[LLM] í”„ë¡œì„¸ì„œê°€ ì´ë¯¸ ì¡´ì¬: {session_id[:8]}")
            return self.processors[session_id]

        processor = LLMProcessor(session_id)
        self.processors[session_id] = processor
        logger.info(f"[LLM] ìƒˆ í”„ë¡œì„¸ì„œ ìƒì„±: {session_id[:8]}")
        return processor

    async def remove_processor(self, session_id: str):
        """LLM í”„ë¡œì„¸ì„œ ì œê±°"""
        if session_id in self.processors:
            await self.processors[session_id].cleanup()
            del self.processors[session_id]
            logger.info(f"[LLM] í”„ë¡œì„¸ì„œ ì œê±°: {session_id[:8]}")

    async def cleanup_all(self):
        """ëª¨ë“  LLM í”„ë¡œì„¸ì„œ ì •ë¦¬"""
        cleanup_tasks = [processor.cleanup() for processor in self.processors.values()]
        if cleanup_tasks:
            await asyncio.gather(*cleanup_tasks, return_exceptions=True)
        self.processors.clear()
        logger.info("[LLM] ëª¨ë“  í”„ë¡œì„¸ì„œ ì •ë¦¬ ì™„ë£Œ")


# ì „ì—­ LLM ë§¤ë‹ˆì €
llm_manager = LLMManager()